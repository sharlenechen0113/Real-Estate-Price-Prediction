{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_csv_only.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharlenechen0113/Real-Estate-Price-Prediction/blob/main/model_csv_only.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8WyP764oIcm"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLQUIvtkrJeb"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing, metrics, model_selection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6DjvqmH_GLI"
      },
      "source": [
        "USE_GPU = True\n",
        "\n",
        "dtype = torch.float # we will be using float throughout this tutorial\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Constant to control how frequently we print train loss\n",
        "print_every = 100\n",
        "\n",
        "print('using device:', device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccHF8JGeenOE"
      },
      "source": [
        "train_mean = 0.0\n",
        "train_std = 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHfoZKWkMaIF"
      },
      "source": [
        "nan_cache = {}\n",
        "def data_preprocess(filename,mode='Train'):\n",
        "  data = pd.read_csv(filename)\n",
        "  if mode == 'Train':\n",
        "    # split data to train, validation and test datsets\n",
        "    train_data, val_data = model_selection.train_test_split(data,test_size = 0.2)\n",
        "    means = train_data.mean() # to use mean or other indicators to fillna?\n",
        "    train_data = train_data.fillna(means)\n",
        "    val_data = val_data.fillna(means)\n",
        "    for column in data:\n",
        "      nan_cache[column] = means\n",
        "    train_y = train_data.pop('unit_price')\n",
        "    val_y = val_data.pop('unit_price')\n",
        "    return train_data, val_data, train_y, val_y\n",
        "  elif mode == 'Test':\n",
        "    real_test_data = data.fillna(nan_cache)\n",
        "    return real_test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtz_4XqiljNn"
      },
      "source": [
        "FILE = '/content/drive/MyDrive/SC201_Final_Project/Data/final_data_0830.csv' # your file path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSayswidD9Ks"
      },
      "source": [
        "train_data, val_data, train_y, val_y = data_preprocess(FILE,mode='Train')\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx26zfLSm5LM"
      },
      "source": [
        "# standardize data\n",
        "train_mean = 207710.376\n",
        "train_std = 79157.365\n",
        "to_drop = ['lat','lng','lat_lng','unit_berth_price','compartmented','management_committee','floors_area','establishment','clothing_store','home_goods_store','store','local_government_office','university','natural_feature','health','tourist_attraction','transit_station','food']\n",
        "for drop_items in to_drop:\n",
        "  train_data.pop(drop_items)\n",
        "  val_data.pop(drop_items)\n",
        "\n",
        "print(train_data.columns)\n",
        "print(val_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7Uv1R_F-r80"
      },
      "source": [
        "# load images to dictionary\n",
        "def load_imgs(lat_lng):\n",
        "  image = Image.open('/content/drive/MyDrive/SC201_Final_Project/images/map/{}.png'.format(lat_lng)).convert('RGB')\n",
        "  image = T.ToTensor()(image)\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCONIJ4cf_S7"
      },
      "source": [
        "class HousingDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,dataset,labels):\n",
        "    self.y_train = torch.tensor(labels.values,dtype=torch.float32).view(len(labels),1)\n",
        "    self.x_train = torch.tensor(dataset.values,dtype=torch.float32)\n",
        "    print(\"HousingDataset shape {}, {}\".format(self.x_train.shape, self.y_train.shape))\n",
        "  def __len__(self):\n",
        "    return len(self.y_train)\n",
        "  def __getitem__(self,idx):\n",
        "    return self.x_train[idx], self.y_train[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM3Du4otxRXk"
      },
      "source": [
        "# Datasets load training examples one at a time, so we wrap each Dataset in a \n",
        "# DataLoader which iterates through the Dataset and forms minibatches. We divide\n",
        "# the training set into train and val sets by passing a Sampler object to the\n",
        "# DataLoader telling how it should sample from the underlying Dataset.\n",
        "# This is for after map data is loaded into the features, so shuffling is done\n",
        "BATCH_SIZE = 32\n",
        "def form_minibatch(data,mode='Train'):\n",
        "  if mode == 'Train':\n",
        "    data = DataLoader(data,batch_size=BATCH_SIZE,shuffle=True)\n",
        "  elif mode == 'Test':\n",
        "    data = DataLoader(data,batch_size=BATCH_SIZE)\n",
        "  return data\n",
        "\n",
        "train_dl = form_minibatch(HousingDataset(train_data,train_y))\n",
        "eval_dl = form_minibatch(HousingDataset(val_data,val_y),mode='Test')\n",
        "print(\"training size {}, eval size {}\".format(len(train_dl), len(eval_dl)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIyfT7CWtmLL"
      },
      "source": [
        "# model = MyModel()\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self,input_size):\n",
        "        super().__init__()\n",
        "        self.nn = nn.Sequential(\n",
        "        nn.Linear(input_size, 60),\n",
        "        nn.BatchNorm1d(60),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(60, 30),\n",
        "        nn.BatchNorm1d(30),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(30, 15),\n",
        "        nn.BatchNorm1d(15),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(15, 8),\n",
        "        nn.BatchNorm1d(8),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(8, 1),\n",
        "        )\n",
        "        # epoch 25-29\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self,x1):\n",
        "        x1 = self.nn(x1)\n",
        "        return x1\n",
        "\n",
        "net = MyModel(72)\n",
        "optimizer = torch.optim.Adam(net.parameters(),lr=1e-5,weight_decay=0.99)\n",
        "optimizer_SGD_momentum = torch.optim.SGD(net.parameters(),lr=1e-5,momentum=0.9,weight_decay=0.98)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dstr6LhxqYi7"
      },
      "source": [
        "loss_history=[]\n",
        "val_loss=[]\n",
        "def train(data1, data2, model, optimizer, epochs=1):\n",
        "    \"\"\"\n",
        "    Train a model on real estate data property features \n",
        "    and extracte map features using the PyTorch Module API.\n",
        "    \n",
        "    Inputs:\n",
        "    - data1: training data\n",
        "    - data2: validation data\n",
        "    - model: A PyTorch Module giving the model to train.\n",
        "    - optimizer: An Optimizer object we will use to train the model\n",
        "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
        "    \n",
        "    Returns: Nothing, but prints model accuracies during training.\n",
        "    \"\"\"\n",
        "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    criterion = nn.L1Loss()\n",
        "    for e in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for t, (x1,y) in enumerate(data1):\n",
        "            model.train()  # put model to training mode\n",
        "            x1 = x1.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=dtype)\n",
        "            \n",
        "            # Zero out all of the gradients for the variables which the optimizer\n",
        "            # will update.\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            scores = model(x1)\n",
        "\n",
        "            loss = criterion(scores,y)\n",
        "            running_loss += loss.item()\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            # This is the backwards pass: compute the gradient of the loss with\n",
        "            # respect to each  parameter of the model.\n",
        "            loss.backward()\n",
        "\n",
        "            # Actually update the parameters of the model using the gradients\n",
        "            # computed by the backwards pass.\n",
        "            optimizer.step()\n",
        "            \n",
        "            if t % 100 == 0:\n",
        "                training_loss = running_loss/len(data1)\n",
        "                print('Iteration %d, loss = %.4f' % (t, training_loss))\n",
        "                print('Checking accuracy on validation set')\n",
        "                eval_loss, eval_l1_loss = check_accuracy(data2, model)\n",
        "                print(\"Epoch {}, Eval loss: {}, l1 loss: {}\".format(e, eval_loss, eval_l1_loss))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D14y1dsq18C"
      },
      "source": [
        "def check_accuracy(data, model):\n",
        "    criterion = nn.L1Loss() \n",
        "    total_loss = 0.0\n",
        "    total_l1_loss = 0.0\n",
        "    model.eval()  # set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "      for x1,y in data:\n",
        "        x1 = x1.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "        y = y.to(device=device, dtype=dtype)\n",
        "        scores = model(x1)\n",
        "        loss = criterion(scores,y)\n",
        "        total_loss += loss.item()\n",
        "        ground_truth_price = y * train_std + train_mean\n",
        "        predicted_price = scores * train_std + train_mean\n",
        "        total_l1_loss += abs(ground_truth_price - predicted_price)\n",
        "      val_loss.append(total_l1_loss.data[0][0]/len(data))\n",
        "      return total_loss/len(data), total_l1_loss.data[0][0]/len(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1HviY5VJZEg"
      },
      "source": [
        "train(train_dl, eval_dl, net,optimizer,epochs=28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGnBRR0pW4I4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(len(loss_history)),loss_history)\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('training loss')\n",
        "plt.title('Training Loss history')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFCnA8DST4Jp"
      },
      "source": [
        "plt.plot(range(len(val_loss)),val_loss)\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('val_l1_loss')\n",
        "plt.title('Validation Loss History')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}